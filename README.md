# Enhancing App Review Feature Extraction through Fine-Tuned Generative LLMs and Multi-Node Workflow Design

## Overview

This project enhances app review feature extraction using fine-tuned generative language models and multi-node workflow design.

## Structure

```
- code
  - fine-tuning
    - exec_finetuning.py
    - generate_finetuning_input_data.py
  - single-node
    - evaluation.py
    - feature_extraction.py
  - multi-node
    - evaluator_optimizer_dft_evaluation_2_2.py
    - evaluator_optimizer_dft_extraction_2_2.py
    - evaluator_optimizer_evaluation_2_1.py
    - evaluator_optimizer_extraction_2_1.py
    - parallelization_coordinator_tmp1_dft_evaluation_3_2.py
    - parallelization_coordinator_tmp1_dft_extraction_3_2.py
    - parallelization_coordinator_tmp1_evaluation_3_1.py
    - parallelization_coordinator_tmp1_extraction_3_1.py
    - parallelization_coordinator_views_evaluation_3_3.py
    - parallelization_coordinator_views_extraction_3_3.py
  - utils
    - conll_to_json_converter.py
    - make_dir.py
  - prompt.py
  - requirements.txt
- README.md
- .python-version

# This directory is not included in this repository. It will be generated by the following process.
- data
  - in-domain
    - bin0
      - evaluation_result
      - feature_extracted_data
      - fine_tuning_data
      - formatted_original_data
        - test-set.json
        - train-set.json
      - original_data
        - test-set.txt (CONLL format)
        - train-set.txt (CONLL format)
    - bin1
      ...
    - bin9
  - out-of-domain
    - COMMUNICATION
      ...
    - WEATHER
```

## 1. Data Preparation

- Use the dataset provided by [T-FREX](https://github.com/nlp4se/t-frex/tree/main/data/T-FREX)
- The dataset contains in-domain and out-of-domain categories, each with 10 subcategories:
  - in-domain: bin0â€¦bin9
  - out-of-domain: "COMMUNICATION", "HEALTH_AND_FITNESS", "LIFESTYLE", "MAPS_AND_NAVIGATION", "PERSONALIZATION", "PRODUCTIVITY", "SOCIAL", "TOOLS", "TRAVEL_AND_LOCAL", "WEATHER"
- Run `make_dir.py` to create the required directory structure under `./data`
- Place `test-set.txt` and `train-set.txt` in the `original_data` directory (see [T-FREX](https://github.com/nlp4se/t-frex/tree/main/data/T-FREX) for original data)
- Run `conll_to_json_converter.py` to convert CoNLL format data to JSON format, which will be saved in the `formatted_original_data` directory

## 2. Fine-tuning

- Run `generate_finetuning_input_data.py` to prepare the dataset for fine-tuning. The system prompt is defined in `code/prompt.py` and referenced in the script
- Execute `exec_finetuning.py` with a specified training file to start fine-tuning. Hyperparameters are automatically optimized based on the dataset, so you can use the default settings

## 3. Inference (Feature Extraction)

### Single-node

- Run `single-node/feature_extraction.py` to perform inference using the fine-tuned model. Specify `INPUT_DIR`, `INPUT_FILE`, `OUTPUT_DIR`, and `MODEL_NAME`
- Extraction results are saved in the `feature_extracted_data` directory

### Multi-node

- Run extraction files (e.g., `multi-node/extractor_optimizer_extraction_2_1.py`) to perform inference. Specify `INPUT_DIR`, `INPUT_FILE`, `OUTPUT_DIR`, and `MODEL_NAME`
- Extraction results are saved in the `feature_extracted_data` directory

## 4. Evaluation

### Single-node

- Run `single-node/evaluation.py` to evaluate extraction results (calculate precision, recall, and F1 scores)
- Evaluation results are saved in the `evaluation_result` directory

### Multi-node

- Run evaluation files (e.g., `evaluator_optimizer_evaluation_2_1.py`) to evaluate extraction results (calculate precision, recall, and F1 scores)
- Evaluation results are saved in the `evaluation_result` directory